---
permalink: /papers/
title: "Do you even read bro? "
---

## How To Make Yourself Miserable:

<div style="display: flex; align-items: center; justify-content: space-between; margin-bottom: 40px;">

  <!-- Left side: Image -->
  <div style="flex: 1; margin-right: 20px;">
    <img src="./How_to_make_yourself_miserable.png" alt="Outfit 1 Image" style="max-width: 100%;">
  </div>

  <!-- Right side: Text -->
  <div style="flex: 1; font: mono;" >
  
DISCOVERING THE SECRETS TO UNHAPPINESS

William F. Doverspike, Ph.D.

Drdoverspike.com

  </div>

</div>

---

## Multimodal Sentiment Analysis on Video Streams using Lightweight Deep Neural Networks

<div style="display: flex; align-items: center; justify-content: space-between; margin-bottom: 40px;">

  <!-- Left side: Image -->
  <div style="flex: 1; margin-right: 20px;">
    <img src="./Multimodal.png" alt="Outfit 2 Image" style="max-width: 100%;">
  </div>

  <!-- Right side: Text -->
  <div style="flex: 1; font: mono;" >
  Real-time sentiment analysis on video streams involves classifying a subject’s emotional expressions over time based on visual and/or audio information in the data stream. Sentiment can be analyzed using various modalities such as speech, mouth motion, and facial expression. This paper proposes a deep learning approach based on multiple modalities in which extracted features of an audiovisual data stream are fused in real time for sentiment classification. The proposed system comprises four small deep neural network models that analyze visual features and audio features concurrently. We fuse the visual and audio sentiment features into a single stream and accumulate evidence over time using an exponentially-weighted moving average to make a final prediction. Our work provides a promising solution to the problem of building real-time sentiment analysis systems that have constrained software or hardware capabilities. Experiments on the Ryerson audio- video database of emotional speech (RAVDESS) show that deep audiovisual feature fusion yields substantial improvements over analysis of either single modality. We obtain an accuracy of 90.74%, which is better than baselines of 11.11% – 31.48% on a challenging test dataset.


  </div>

</div>

---
